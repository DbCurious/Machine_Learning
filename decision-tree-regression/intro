A decision tree is a supervised machine learning model used to predict a target by learning decision rules from features. 
As the name suggests, we can think of this model as breaking down our data by making a decision based on asking a series of questions.
Decision-tree algorithm falls under the category of supervised learning algorithms. It works for both continuous as well as categorical output variables.
The branches/edges represent the result of the node and the nodes have either:  
  a.Conditions [Decision Nodes]
  b.Result [End Nodes]
  
Decision tree regression observes features of an object and trains a model in the structure of a tree to predict data in the future to produce meaningful continuous output. 
Continuous output means that the output/result is not discrete, i.e., it is not represented just by a discrete, known set of numbers or values.

# import the regressor 
from sklearn.tree import DecisionTreeRegressor 

# create a regressor object 
regressor = DecisionTreeRegressor(random_state = 0) 

# fit the regressor with X and Y data 
regressor.fit(X, y) 


  
